{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a308e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Shikhar Mohan\n",
    "# Roll No.: 18EC10054\n",
    "# Date: 17/09/2021\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8d8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # breakpoint()\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    return z\n",
    "\n",
    "def dsigmoid(x):\n",
    "    z = sigmoid(x)\n",
    "    return z*(1-z)\n",
    "\n",
    "def data_normalize(X):\n",
    "    for i in range(9):\n",
    "        X[:,i] = (X[:,i] - X[:,i].mean())/X[:,i].std()\n",
    "    return X\n",
    "\n",
    "def data_onehot(x,classes):\n",
    "    y = [0,0,0]\n",
    "    y[x] = 1\n",
    "    return y\n",
    "\n",
    "def data_helper(path):\n",
    "    data = pd.read_csv(path).to_numpy()\n",
    "    X = data[:,:9].astype(np.float64)\n",
    "    y = data[:,9]\n",
    "    classes = len(np.unique(y))\n",
    "    label_dict = dict(zip(np.unique(y), range(classes)))\n",
    "    for idx in range(len(y)): y[idx] = label_dict[y[idx]]\n",
    "    out = [0 for i in range(len(y))]\n",
    "    for idx in range(len(y)): out[idx] = data_onehot(y[idx],classes)\n",
    "    X = data_normalize(X)\n",
    "    return X,np.asarray(out)\n",
    "\n",
    "def data_scatter_plot(X, y, idx_1 = 2, idx_2 = 3):\n",
    "    \n",
    "    colors = [[] for i in range(len(np,unique(y)))]\n",
    "    for d,l in zip(X, y):\n",
    "        label = np.argmax(l)\n",
    "        if label not in color_dict: color_dict[label] = []\n",
    "        color_dict[label].append(d)\n",
    "        \n",
    "    for k,v in color_dict.items():\n",
    "        plt.scatter(np.array(v)[:,idx_1],np.array(v)[:,idx_2], label = k)\n",
    "        \n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae24500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \"\"\"\n",
    "    Two layer neural network (one hidden, one final) with sigmoid activations for both layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, std=3e-4):\n",
    "        # Parse the config file\n",
    "        in_size = config['in_size']\n",
    "        hid_size = config['hid_size']\n",
    "        out_size = config['out_size']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.num_iters = config['epochs']\n",
    "        self.lr = config['learning_rate']\n",
    "\n",
    "        # Create the weight and bias vectors\n",
    "        self.W1 = std * np.random.randn(in_size, hid_size)\n",
    "        self.b1 = np.zeros(hid_size)\n",
    "        self.W2 = std * np.random.randn(hid_size, out_size)\n",
    "        self.b2 = np.zeros(out_size)\n",
    "\n",
    "    def forward(self, X, y = None):\n",
    "        \"\"\"\n",
    "        This function takes as input the data and outputs activations is y is not given\n",
    "        If y is given, it returns loss and gradients.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.W1, self.b1\n",
    "        W2, b2 = self.W2, self.b2\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Computing the forward pass\n",
    "        x1 = X@W1 + b1\n",
    "        x_h = sigmoid(x1)\n",
    "        y1 = x_h@W2 + b2\n",
    "        y_hat = sigmoid(y1)\n",
    "\n",
    "        if y is None: return y_hat\n",
    "        \n",
    "        loss = np.mean((y_hat - y)**2)/2\n",
    "        # Computing the backward pass via chain rule\n",
    "        dloss = (y_hat - y)\n",
    "        dy_hat = dsigmoid(y1) * dloss\n",
    "        dW2 = x1.T@dy_hat\n",
    "        db2 = np.sum(dy_hat,axis=0)\n",
    "\n",
    "        dx_h = dy_hat@(W2.T)\n",
    "        dx1 = dx_h*dsigmoid(x1)\n",
    "        dW1 = X.T@dx1\n",
    "        db1 = np.sum(dx1,axis=0)\n",
    "\n",
    "        grads = (dW1, db1, dW2, db2)\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_t, y_t):\n",
    "        \"\"\"\n",
    "        Method for training model on given data.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "\n",
    "        self.losses, self.train_accs, self.test_accs = [],[],[]\n",
    "        \n",
    "        for it in range(1,self.num_iters+1):\n",
    "            \n",
    "            loss, grads = self.forward(X, y=y)\n",
    "            dW1, db1, dW2, db2 = grads\n",
    "            self.W1 -= self.lr*dW1\n",
    "            self.b1 -= self.lr*np.squeeze(db1)\n",
    "            self.W2 -= self.lr*dW2\n",
    "            self.b2 -= self.lr*np.squeeze(db2)\n",
    "\n",
    "            train_acc = (self.predict(X) == np.argmax(y,axis=1)).mean()\n",
    "            test_acc = (self.predict(X_t) == np.argmax(y_t,axis=1)).mean()\n",
    "\n",
    "            if it % 50:\n",
    "                print(\"Iter.%d:\\t%f loss\\t%f train accuracy\\t%f test accuracy\" % (it, loss, train_acc, test_acc))\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            self.train_accs.append(train_acc)\n",
    "            self.test_accs.append(test_acc)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = self.forward(X)\n",
    "        return np.argmax(y,axis=1)\n",
    "\n",
    "    def visualize(self, X, y):\n",
    "        mean, std = X.mean(axis = 0), X.std(axis = 0)\n",
    "\n",
    "        plt.title('Scatter Plot with two features before Z-normalisation')\n",
    "        data_scatter_plot(X, y)\n",
    "\n",
    "        train_data = (X-mean)/std\n",
    "\n",
    "        plt.title('Scatter Plot with two features after Z-normalisation')\n",
    "        data_scatter_plot(X, y)\n",
    "        \n",
    "        \n",
    "        # Visualize Cost vs Epoch\n",
    "        plt.plot(self.losses)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.title('Training Loss vs Epochs')\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize Training and Testing accuracy\n",
    "        plt.plot(self.train_accs, label = 'Train Acc')\n",
    "        plt.plot(self.test_accs, label = 'Test Acc')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.title('Accuracy vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f106711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.1:\t0.125008 loss\t0.368135 train accuracy\t0.358852 test accuracy\n",
      "Iter.2:\t0.117826 loss\t0.368135 train accuracy\t0.358852 test accuracy\n",
      "Iter.3:\t0.111071 loss\t0.368135 train accuracy\t0.358852 test accuracy\n",
      "Iter.4:\t0.110896 loss\t0.368135 train accuracy\t0.358852 test accuracy\n",
      "Iter.5:\t0.110469 loss\t0.511175 train accuracy\t0.488038 test accuracy\n",
      "Iter.6:\t0.105741 loss\t0.532886 train accuracy\t0.525359 test accuracy\n",
      "Iter.7:\t0.091913 loss\t0.534483 train accuracy\t0.533014 test accuracy\n",
      "Iter.8:\t0.092166 loss\t0.537356 train accuracy\t0.524402 test accuracy\n",
      "Iter.9:\t0.096629 loss\t0.539591 train accuracy\t0.518660 test accuracy\n",
      "Iter.10:\t0.104729 loss\t0.534483 train accuracy\t0.512919 test accuracy\n",
      "Iter.11:\t0.093417 loss\t0.531928 train accuracy\t0.514833 test accuracy\n",
      "Iter.12:\t0.096068 loss\t0.537676 train accuracy\t0.518660 test accuracy\n",
      "Iter.13:\t0.100988 loss\t0.538633 train accuracy\t0.525359 test accuracy\n",
      "Iter.14:\t0.119957 loss\t0.534483 train accuracy\t0.485167 test accuracy\n",
      "Iter.15:\t0.104194 loss\t0.340677 train accuracy\t0.322488 test accuracy\n",
      "Iter.16:\t0.123833 loss\t0.536079 train accuracy\t0.528230 test accuracy\n",
      "Iter.17:\t0.130771 loss\t0.538633 train accuracy\t0.523445 test accuracy\n",
      "Iter.18:\t0.110476 loss\t0.507663 train accuracy\t0.513876 test accuracy\n",
      "Iter.19:\t0.140274 loss\t0.537037 train accuracy\t0.511005 test accuracy\n",
      "Iter.20:\t0.133102 loss\t0.541507 train accuracy\t0.507177 test accuracy\n",
      "Iter.21:\t0.113298 loss\t0.467752 train accuracy\t0.517703 test accuracy\n",
      "Iter.22:\t0.122413 loss\t0.520115 train accuracy\t0.502392 test accuracy\n",
      "Iter.23:\t0.116680 loss\t0.512133 train accuracy\t0.514833 test accuracy\n",
      "Iter.24:\t0.152884 loss\t0.511814 train accuracy\t0.515789 test accuracy\n",
      "Iter.25:\t0.140662 loss\t0.533206 train accuracy\t0.494737 test accuracy\n",
      "Iter.26:\t0.143271 loss\t0.515645 train accuracy\t0.517703 test accuracy\n",
      "Iter.27:\t0.107287 loss\t0.538953 train accuracy\t0.516746 test accuracy\n",
      "Iter.28:\t0.139955 loss\t0.538953 train accuracy\t0.516746 test accuracy\n",
      "Iter.29:\t0.121898 loss\t0.538633 train accuracy\t0.503349 test accuracy\n",
      "Iter.30:\t0.118450 loss\t0.507982 train accuracy\t0.489952 test accuracy\n",
      "Iter.31:\t0.119908 loss\t0.356641 train accuracy\t0.354067 test accuracy\n",
      "Iter.32:\t0.134364 loss\t0.348978 train accuracy\t0.324402 test accuracy\n",
      "Iter.33:\t0.145140 loss\t0.534163 train accuracy\t0.516746 test accuracy\n",
      "Iter.34:\t0.128688 loss\t0.541826 train accuracy\t0.532057 test accuracy\n",
      "Iter.35:\t0.124795 loss\t0.539911 train accuracy\t0.524402 test accuracy\n",
      "Iter.36:\t0.116228 loss\t0.544381 train accuracy\t0.522488 test accuracy\n",
      "Iter.37:\t0.117854 loss\t0.518199 train accuracy\t0.520574 test accuracy\n",
      "Iter.38:\t0.121093 loss\t0.535760 train accuracy\t0.521531 test accuracy\n",
      "Iter.39:\t0.129330 loss\t0.545338 train accuracy\t0.520574 test accuracy\n",
      "Iter.40:\t0.103395 loss\t0.517880 train accuracy\t0.525359 test accuracy\n",
      "Iter.41:\t0.114820 loss\t0.541188 train accuracy\t0.525359 test accuracy\n",
      "Iter.42:\t0.106279 loss\t0.517880 train accuracy\t0.531100 test accuracy\n",
      "Iter.43:\t0.109177 loss\t0.545019 train accuracy\t0.519617 test accuracy\n",
      "Iter.44:\t0.110143 loss\t0.532567 train accuracy\t0.521531 test accuracy\n",
      "Iter.45:\t0.101836 loss\t0.534483 train accuracy\t0.524402 test accuracy\n",
      "Iter.46:\t0.116043 loss\t0.543423 train accuracy\t0.525359 test accuracy\n",
      "Iter.47:\t0.106996 loss\t0.516603 train accuracy\t0.534928 test accuracy\n",
      "Iter.48:\t0.105434 loss\t0.543423 train accuracy\t0.525359 test accuracy\n",
      "Iter.49:\t0.108850 loss\t0.521711 train accuracy\t0.529187 test accuracy\n",
      "Iter.51:\t0.107341 loss\t0.540230 train accuracy\t0.519617 test accuracy\n",
      "Iter.52:\t0.101348 loss\t0.545019 train accuracy\t0.527273 test accuracy\n",
      "Iter.53:\t0.108469 loss\t0.548531 train accuracy\t0.509091 test accuracy\n",
      "Iter.54:\t0.100739 loss\t0.478608 train accuracy\t0.533971 test accuracy\n",
      "Iter.55:\t0.102440 loss\t0.530651 train accuracy\t0.521531 test accuracy\n",
      "Iter.56:\t0.117870 loss\t0.507344 train accuracy\t0.527273 test accuracy\n",
      "Iter.57:\t0.132423 loss\t0.507982 train accuracy\t0.521531 test accuracy\n",
      "Iter.58:\t0.151462 loss\t0.538633 train accuracy\t0.518660 test accuracy\n",
      "Iter.59:\t0.125952 loss\t0.537676 train accuracy\t0.512919 test accuracy\n",
      "Iter.60:\t0.120081 loss\t0.538314 train accuracy\t0.518660 test accuracy\n",
      "Iter.61:\t0.101910 loss\t0.516284 train accuracy\t0.518660 test accuracy\n",
      "Iter.62:\t0.125603 loss\t0.513729 train accuracy\t0.522488 test accuracy\n",
      "Iter.63:\t0.136182 loss\t0.516603 train accuracy\t0.518660 test accuracy\n",
      "Iter.64:\t0.131440 loss\t0.519476 train accuracy\t0.519617 test accuracy\n",
      "Iter.65:\t0.116400 loss\t0.521711 train accuracy\t0.523445 test accuracy\n",
      "Iter.66:\t0.130914 loss\t0.545019 train accuracy\t0.517703 test accuracy\n",
      "Iter.67:\t0.099958 loss\t0.516603 train accuracy\t0.513876 test accuracy\n",
      "Iter.68:\t0.132674 loss\t0.518519 train accuracy\t0.519617 test accuracy\n",
      "Iter.69:\t0.129975 loss\t0.518199 train accuracy\t0.519617 test accuracy\n",
      "Iter.70:\t0.118591 loss\t0.520115 train accuracy\t0.525359 test accuracy\n",
      "Iter.71:\t0.131434 loss\t0.517241 train accuracy\t0.518660 test accuracy\n",
      "Iter.72:\t0.115057 loss\t0.524585 train accuracy\t0.517703 test accuracy\n",
      "Iter.73:\t0.123110 loss\t0.534483 train accuracy\t0.513876 test accuracy\n",
      "Iter.74:\t0.106728 loss\t0.543103 train accuracy\t0.513876 test accuracy\n",
      "Iter.75:\t0.113579 loss\t0.545019 train accuracy\t0.487081 test accuracy\n",
      "Iter.76:\t0.101176 loss\t0.549170 train accuracy\t0.511005 test accuracy\n",
      "Iter.77:\t0.101103 loss\t0.546935 train accuracy\t0.485167 test accuracy\n",
      "Iter.78:\t0.098420 loss\t0.542465 train accuracy\t0.512919 test accuracy\n",
      "Iter.79:\t0.099407 loss\t0.540868 train accuracy\t0.511962 test accuracy\n",
      "Iter.80:\t0.095899 loss\t0.538633 train accuracy\t0.510048 test accuracy\n",
      "Iter.81:\t0.101696 loss\t0.548851 train accuracy\t0.518660 test accuracy\n",
      "Iter.82:\t0.110796 loss\t0.544700 train accuracy\t0.507177 test accuracy\n",
      "Iter.83:\t0.113361 loss\t0.523308 train accuracy\t0.471770 test accuracy\n",
      "Iter.84:\t0.100689 loss\t0.538314 train accuracy\t0.523445 test accuracy\n",
      "Iter.85:\t0.101916 loss\t0.557152 train accuracy\t0.530144 test accuracy\n",
      "Iter.86:\t0.092724 loss\t0.554278 train accuracy\t0.524402 test accuracy\n",
      "Iter.87:\t0.091814 loss\t0.549170 train accuracy\t0.533971 test accuracy\n",
      "Iter.88:\t0.091426 loss\t0.545977 train accuracy\t0.520574 test accuracy\n",
      "Iter.89:\t0.096227 loss\t0.544700 train accuracy\t0.542584 test accuracy\n",
      "Iter.90:\t0.096618 loss\t0.533206 train accuracy\t0.523445 test accuracy\n",
      "Iter.91:\t0.097599 loss\t0.526181 train accuracy\t0.528230 test accuracy\n",
      "Iter.92:\t0.093720 loss\t0.531290 train accuracy\t0.512919 test accuracy\n",
      "Iter.93:\t0.092598 loss\t0.517241 train accuracy\t0.527273 test accuracy\n",
      "Iter.94:\t0.094268 loss\t0.531290 train accuracy\t0.525359 test accuracy\n",
      "Iter.95:\t0.102173 loss\t0.543742 train accuracy\t0.526316 test accuracy\n",
      "Iter.96:\t0.099494 loss\t0.530971 train accuracy\t0.533014 test accuracy\n",
      "Iter.97:\t0.101281 loss\t0.546616 train accuracy\t0.537799 test accuracy\n",
      "Iter.98:\t0.093244 loss\t0.550447 train accuracy\t0.525359 test accuracy\n",
      "Iter.99:\t0.090204 loss\t0.541826 train accuracy\t0.534928 test accuracy\n",
      "Iter.101:\t0.091030 loss\t0.541188 train accuracy\t0.537799 test accuracy\n",
      "Iter.102:\t0.090436 loss\t0.540868 train accuracy\t0.524402 test accuracy\n",
      "Iter.103:\t0.093598 loss\t0.531290 train accuracy\t0.531100 test accuracy\n",
      "Iter.104:\t0.090184 loss\t0.550766 train accuracy\t0.517703 test accuracy\n",
      "Iter.105:\t0.092261 loss\t0.544061 train accuracy\t0.533014 test accuracy\n",
      "Iter.106:\t0.090522 loss\t0.547893 train accuracy\t0.520574 test accuracy\n",
      "Iter.107:\t0.095635 loss\t0.551724 train accuracy\t0.539713 test accuracy\n",
      "Iter.108:\t0.089520 loss\t0.549170 train accuracy\t0.526316 test accuracy\n",
      "Iter.109:\t0.089041 loss\t0.552682 train accuracy\t0.529187 test accuracy\n",
      "Iter.110:\t0.088099 loss\t0.545019 train accuracy\t0.530144 test accuracy\n",
      "Iter.111:\t0.087929 loss\t0.542784 train accuracy\t0.530144 test accuracy\n",
      "Iter.112:\t0.088048 loss\t0.534163 train accuracy\t0.524402 test accuracy\n",
      "Iter.113:\t0.088201 loss\t0.537676 train accuracy\t0.529187 test accuracy\n",
      "Iter.114:\t0.088401 loss\t0.529693 train accuracy\t0.537799 test accuracy\n",
      "Iter.115:\t0.089608 loss\t0.534163 train accuracy\t0.522488 test accuracy\n",
      "Iter.116:\t0.097057 loss\t0.523627 train accuracy\t0.542584 test accuracy\n",
      "Iter.117:\t0.098318 loss\t0.504151 train accuracy\t0.518660 test accuracy\n",
      "Iter.118:\t0.100964 loss\t0.520754 train accuracy\t0.523445 test accuracy\n",
      "Iter.119:\t0.105293 loss\t0.433908 train accuracy\t0.409569 test accuracy\n",
      "Iter.120:\t0.114443 loss\t0.523946 train accuracy\t0.533014 test accuracy\n",
      "Iter.121:\t0.110435 loss\t0.542146 train accuracy\t0.520574 test accuracy\n",
      "Iter.122:\t0.103639 loss\t0.549489 train accuracy\t0.522488 test accuracy\n",
      "Iter.123:\t0.091292 loss\t0.536718 train accuracy\t0.516746 test accuracy\n",
      "Iter.124:\t0.093042 loss\t0.527458 train accuracy\t0.527273 test accuracy\n",
      "Iter.125:\t0.095816 loss\t0.539911 train accuracy\t0.517703 test accuracy\n",
      "Iter.126:\t0.091308 loss\t0.552682 train accuracy\t0.513876 test accuracy\n",
      "Iter.127:\t0.091294 loss\t0.521392 train accuracy\t0.534928 test accuracy\n",
      "Iter.128:\t0.091634 loss\t0.538953 train accuracy\t0.518660 test accuracy\n",
      "Iter.129:\t0.112013 loss\t0.541188 train accuracy\t0.514833 test accuracy\n",
      "Iter.130:\t0.106508 loss\t0.527458 train accuracy\t0.515789 test accuracy\n",
      "Iter.131:\t0.100575 loss\t0.520754 train accuracy\t0.512919 test accuracy\n",
      "Iter.132:\t0.090370 loss\t0.533206 train accuracy\t0.526316 test accuracy\n",
      "Iter.133:\t0.091409 loss\t0.543103 train accuracy\t0.521531 test accuracy\n",
      "Iter.134:\t0.088188 loss\t0.545019 train accuracy\t0.518660 test accuracy\n",
      "Iter.135:\t0.090663 loss\t0.522031 train accuracy\t0.529187 test accuracy\n",
      "Iter.136:\t0.094166 loss\t0.542465 train accuracy\t0.517703 test accuracy\n",
      "Iter.137:\t0.109467 loss\t0.544381 train accuracy\t0.524402 test accuracy\n",
      "Iter.138:\t0.098611 loss\t0.530332 train accuracy\t0.526316 test accuracy\n",
      "Iter.139:\t0.095997 loss\t0.520115 train accuracy\t0.529187 test accuracy\n",
      "Iter.140:\t0.095057 loss\t0.521711 train accuracy\t0.551196 test accuracy\n",
      "Iter.141:\t0.090384 loss\t0.546616 train accuracy\t0.526316 test accuracy\n",
      "Iter.142:\t0.088796 loss\t0.541826 train accuracy\t0.529187 test accuracy\n",
      "Iter.143:\t0.088836 loss\t0.548851 train accuracy\t0.522488 test accuracy\n",
      "Iter.144:\t0.093543 loss\t0.544381 train accuracy\t0.525359 test accuracy\n",
      "Iter.145:\t0.088030 loss\t0.556833 train accuracy\t0.529187 test accuracy\n",
      "Iter.146:\t0.090630 loss\t0.520754 train accuracy\t0.537799 test accuracy\n",
      "Iter.147:\t0.091996 loss\t0.532886 train accuracy\t0.534928 test accuracy\n",
      "Iter.148:\t0.101450 loss\t0.544700 train accuracy\t0.534928 test accuracy\n",
      "Iter.149:\t0.096421 loss\t0.548851 train accuracy\t0.532057 test accuracy\n",
      "Iter.151:\t0.088909 loss\t0.534802 train accuracy\t0.532057 test accuracy\n",
      "Iter.152:\t0.089347 loss\t0.528736 train accuracy\t0.532057 test accuracy\n",
      "Iter.153:\t0.088166 loss\t0.543103 train accuracy\t0.532057 test accuracy\n",
      "Iter.154:\t0.088142 loss\t0.540868 train accuracy\t0.533014 test accuracy\n",
      "Iter.155:\t0.089531 loss\t0.538953 train accuracy\t0.534928 test accuracy\n",
      "Iter.156:\t0.096422 loss\t0.556833 train accuracy\t0.538756 test accuracy\n",
      "Iter.157:\t0.089565 loss\t0.538633 train accuracy\t0.540670 test accuracy\n",
      "Iter.158:\t0.089473 loss\t0.542784 train accuracy\t0.536842 test accuracy\n",
      "Iter.159:\t0.094013 loss\t0.543103 train accuracy\t0.545455 test accuracy\n",
      "Iter.160:\t0.087693 loss\t0.541826 train accuracy\t0.529187 test accuracy\n",
      "Iter.161:\t0.088297 loss\t0.541507 train accuracy\t0.556938 test accuracy\n",
      "Iter.162:\t0.091403 loss\t0.545338 train accuracy\t0.529187 test accuracy\n",
      "Iter.163:\t0.087539 loss\t0.530013 train accuracy\t0.557895 test accuracy\n",
      "Iter.164:\t0.089000 loss\t0.540549 train accuracy\t0.535885 test accuracy\n",
      "Iter.165:\t0.089178 loss\t0.524266 train accuracy\t0.543541 test accuracy\n",
      "Iter.166:\t0.093215 loss\t0.544381 train accuracy\t0.536842 test accuracy\n",
      "Iter.167:\t0.087706 loss\t0.549808 train accuracy\t0.560766 test accuracy\n",
      "Iter.168:\t0.088117 loss\t0.533525 train accuracy\t0.532057 test accuracy\n",
      "Iter.169:\t0.090259 loss\t0.538953 train accuracy\t0.515789 test accuracy\n",
      "Iter.170:\t0.099665 loss\t0.552682 train accuracy\t0.544498 test accuracy\n",
      "Iter.171:\t0.093612 loss\t0.548212 train accuracy\t0.528230 test accuracy\n",
      "Iter.172:\t0.087990 loss\t0.551086 train accuracy\t0.548325 test accuracy\n",
      "Iter.173:\t0.088319 loss\t0.545019 train accuracy\t0.528230 test accuracy\n",
      "Iter.174:\t0.090983 loss\t0.541188 train accuracy\t0.569378 test accuracy\n",
      "Iter.175:\t0.087266 loss\t0.548851 train accuracy\t0.536842 test accuracy\n",
      "Iter.176:\t0.088181 loss\t0.515326 train accuracy\t0.548325 test accuracy\n",
      "Iter.177:\t0.090130 loss\t0.546296 train accuracy\t0.518660 test accuracy\n",
      "Iter.178:\t0.097012 loss\t0.522669 train accuracy\t0.546411 test accuracy\n",
      "Iter.179:\t0.089944 loss\t0.538633 train accuracy\t0.538756 test accuracy\n",
      "Iter.180:\t0.088751 loss\t0.523946 train accuracy\t0.546411 test accuracy\n",
      "Iter.181:\t0.088869 loss\t0.540230 train accuracy\t0.530144 test accuracy\n",
      "Iter.182:\t0.088661 loss\t0.542465 train accuracy\t0.556938 test accuracy\n",
      "Iter.183:\t0.091286 loss\t0.540549 train accuracy\t0.528230 test accuracy\n",
      "Iter.184:\t0.087214 loss\t0.551724 train accuracy\t0.564593 test accuracy\n",
      "Iter.185:\t0.087522 loss\t0.550766 train accuracy\t0.529187 test accuracy\n",
      "Iter.186:\t0.087661 loss\t0.547893 train accuracy\t0.548325 test accuracy\n",
      "Iter.187:\t0.090844 loss\t0.547893 train accuracy\t0.533971 test accuracy\n",
      "Iter.188:\t0.086757 loss\t0.553321 train accuracy\t0.548325 test accuracy\n",
      "Iter.189:\t0.087166 loss\t0.554917 train accuracy\t0.529187 test accuracy\n",
      "Iter.190:\t0.087646 loss\t0.549808 train accuracy\t0.534928 test accuracy\n",
      "Iter.191:\t0.092207 loss\t0.553321 train accuracy\t0.539713 test accuracy\n",
      "Iter.192:\t0.087119 loss\t0.552363 train accuracy\t0.547368 test accuracy\n",
      "Iter.193:\t0.086331 loss\t0.550766 train accuracy\t0.543541 test accuracy\n",
      "Iter.194:\t0.086423 loss\t0.557152 train accuracy\t0.540670 test accuracy\n",
      "Iter.195:\t0.086741 loss\t0.547573 train accuracy\t0.533971 test accuracy\n",
      "Iter.196:\t0.088583 loss\t0.544700 train accuracy\t0.522488 test accuracy\n",
      "Iter.197:\t0.095368 loss\t0.539911 train accuracy\t0.565550 test accuracy\n",
      "Iter.198:\t0.093335 loss\t0.533206 train accuracy\t0.516746 test accuracy\n",
      "Iter.199:\t0.091874 loss\t0.521711 train accuracy\t0.537799 test accuracy\n",
      "Iter.201:\t0.090442 loss\t0.529374 train accuracy\t0.540670 test accuracy\n",
      "Iter.202:\t0.092454 loss\t0.544381 train accuracy\t0.532057 test accuracy\n",
      "Iter.203:\t0.088740 loss\t0.547254 train accuracy\t0.555981 test accuracy\n",
      "Iter.204:\t0.089448 loss\t0.539911 train accuracy\t0.518660 test accuracy\n",
      "Iter.205:\t0.089438 loss\t0.559068 train accuracy\t0.542584 test accuracy\n",
      "Iter.206:\t0.093256 loss\t0.553640 train accuracy\t0.535885 test accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.207:\t0.087679 loss\t0.553321 train accuracy\t0.554067 test accuracy\n",
      "Iter.208:\t0.086727 loss\t0.555236 train accuracy\t0.531100 test accuracy\n",
      "Iter.209:\t0.087064 loss\t0.553001 train accuracy\t0.558852 test accuracy\n",
      "Iter.210:\t0.086375 loss\t0.560026 train accuracy\t0.533014 test accuracy\n",
      "Iter.211:\t0.087079 loss\t0.545019 train accuracy\t0.545455 test accuracy\n",
      "Iter.212:\t0.087404 loss\t0.549808 train accuracy\t0.536842 test accuracy\n",
      "Iter.213:\t0.091628 loss\t0.526181 train accuracy\t0.531100 test accuracy\n",
      "Iter.214:\t0.090675 loss\t0.548531 train accuracy\t0.543541 test accuracy\n",
      "Iter.215:\t0.092436 loss\t0.505747 train accuracy\t0.527273 test accuracy\n",
      "Iter.216:\t0.092240 loss\t0.546935 train accuracy\t0.525359 test accuracy\n",
      "Iter.217:\t0.096172 loss\t0.524266 train accuracy\t0.552153 test accuracy\n",
      "Iter.218:\t0.090640 loss\t0.553640 train accuracy\t0.533971 test accuracy\n",
      "Iter.219:\t0.088477 loss\t0.530971 train accuracy\t0.551196 test accuracy\n",
      "Iter.220:\t0.088208 loss\t0.546616 train accuracy\t0.528230 test accuracy\n",
      "Iter.221:\t0.087048 loss\t0.535760 train accuracy\t0.555981 test accuracy\n",
      "Iter.222:\t0.088096 loss\t0.545658 train accuracy\t0.527273 test accuracy\n",
      "Iter.223:\t0.087998 loss\t0.531290 train accuracy\t0.552153 test accuracy\n",
      "Iter.224:\t0.091840 loss\t0.546296 train accuracy\t0.533971 test accuracy\n",
      "Iter.225:\t0.088023 loss\t0.531928 train accuracy\t0.550239 test accuracy\n",
      "Iter.226:\t0.087553 loss\t0.551086 train accuracy\t0.530144 test accuracy\n",
      "Iter.227:\t0.087491 loss\t0.531928 train accuracy\t0.554067 test accuracy\n",
      "Iter.228:\t0.086985 loss\t0.551086 train accuracy\t0.529187 test accuracy\n",
      "Iter.229:\t0.086954 loss\t0.541188 train accuracy\t0.554067 test accuracy\n",
      "Iter.230:\t0.087718 loss\t0.542784 train accuracy\t0.517703 test accuracy\n",
      "Iter.231:\t0.088561 loss\t0.543423 train accuracy\t0.546411 test accuracy\n",
      "Iter.232:\t0.094655 loss\t0.553001 train accuracy\t0.533014 test accuracy\n",
      "Iter.233:\t0.089641 loss\t0.556194 train accuracy\t0.547368 test accuracy\n",
      "Iter.234:\t0.086802 loss\t0.558110 train accuracy\t0.533971 test accuracy\n",
      "Iter.235:\t0.087081 loss\t0.552043 train accuracy\t0.544498 test accuracy\n",
      "Iter.236:\t0.086179 loss\t0.560026 train accuracy\t0.534928 test accuracy\n",
      "Iter.237:\t0.086836 loss\t0.539911 train accuracy\t0.552153 test accuracy\n",
      "Iter.238:\t0.087652 loss\t0.550128 train accuracy\t0.525359 test accuracy\n",
      "Iter.239:\t0.092768 loss\t0.528097 train accuracy\t0.542584 test accuracy\n",
      "Iter.240:\t0.090970 loss\t0.556513 train accuracy\t0.524402 test accuracy\n",
      "Iter.241:\t0.089785 loss\t0.508940 train accuracy\t0.540670 test accuracy\n",
      "Iter.242:\t0.091626 loss\t0.548851 train accuracy\t0.519617 test accuracy\n",
      "Iter.243:\t0.095089 loss\t0.531928 train accuracy\t0.554067 test accuracy\n",
      "Iter.244:\t0.088103 loss\t0.547254 train accuracy\t0.533971 test accuracy\n",
      "Iter.245:\t0.087244 loss\t0.545019 train accuracy\t0.558852 test accuracy\n",
      "Iter.246:\t0.087634 loss\t0.552363 train accuracy\t0.525359 test accuracy\n",
      "Iter.247:\t0.086599 loss\t0.542146 train accuracy\t0.560766 test accuracy\n",
      "Iter.248:\t0.088203 loss\t0.547893 train accuracy\t0.522488 test accuracy\n",
      "Iter.249:\t0.087308 loss\t0.536398 train accuracy\t0.547368 test accuracy\n",
      "Iter.251:\t0.087940 loss\t0.539272 train accuracy\t0.553110 test accuracy\n",
      "Iter.252:\t0.088201 loss\t0.549489 train accuracy\t0.522488 test accuracy\n",
      "Iter.253:\t0.088131 loss\t0.542784 train accuracy\t0.549282 test accuracy\n",
      "Iter.254:\t0.089775 loss\t0.550447 train accuracy\t0.529187 test accuracy\n",
      "Iter.255:\t0.087094 loss\t0.551405 train accuracy\t0.557895 test accuracy\n",
      "Iter.256:\t0.087487 loss\t0.556833 train accuracy\t0.522488 test accuracy\n",
      "Iter.257:\t0.087700 loss\t0.550766 train accuracy\t0.539713 test accuracy\n",
      "Iter.258:\t0.090427 loss\t0.563857 train accuracy\t0.532057 test accuracy\n",
      "Iter.259:\t0.087435 loss\t0.556833 train accuracy\t0.539713 test accuracy\n",
      "Iter.260:\t0.086990 loss\t0.565134 train accuracy\t0.532057 test accuracy\n",
      "Iter.261:\t0.088827 loss\t0.554917 train accuracy\t0.533014 test accuracy\n",
      "Iter.262:\t0.091101 loss\t0.551724 train accuracy\t0.553110 test accuracy\n",
      "Iter.263:\t0.089241 loss\t0.556833 train accuracy\t0.534928 test accuracy\n",
      "Iter.264:\t0.087541 loss\t0.544381 train accuracy\t0.550239 test accuracy\n",
      "Iter.265:\t0.089179 loss\t0.560026 train accuracy\t0.534928 test accuracy\n",
      "Iter.266:\t0.089851 loss\t0.550766 train accuracy\t0.563636 test accuracy\n",
      "Iter.267:\t0.087789 loss\t0.558748 train accuracy\t0.534928 test accuracy\n",
      "Iter.268:\t0.088062 loss\t0.550128 train accuracy\t0.553110 test accuracy\n",
      "Iter.269:\t0.088399 loss\t0.563538 train accuracy\t0.535885 test accuracy\n",
      "Iter.270:\t0.089435 loss\t0.560983 train accuracy\t0.560766 test accuracy\n",
      "Iter.271:\t0.087402 loss\t0.558748 train accuracy\t0.538756 test accuracy\n",
      "Iter.272:\t0.087721 loss\t0.558110 train accuracy\t0.550239 test accuracy\n",
      "Iter.273:\t0.087752 loss\t0.563218 train accuracy\t0.538756 test accuracy\n",
      "Iter.274:\t0.088831 loss\t0.560664 train accuracy\t0.560766 test accuracy\n",
      "Iter.275:\t0.087252 loss\t0.560026 train accuracy\t0.539713 test accuracy\n",
      "Iter.276:\t0.087823 loss\t0.547573 train accuracy\t0.555981 test accuracy\n",
      "Iter.277:\t0.087626 loss\t0.560664 train accuracy\t0.539713 test accuracy\n",
      "Iter.278:\t0.088652 loss\t0.543103 train accuracy\t0.564593 test accuracy\n",
      "Iter.279:\t0.087607 loss\t0.554917 train accuracy\t0.544498 test accuracy\n",
      "Iter.280:\t0.088460 loss\t0.533844 train accuracy\t0.555024 test accuracy\n",
      "Iter.281:\t0.088131 loss\t0.554917 train accuracy\t0.533014 test accuracy\n",
      "Iter.282:\t0.089074 loss\t0.540549 train accuracy\t0.558852 test accuracy\n",
      "Iter.283:\t0.087243 loss\t0.556513 train accuracy\t0.539713 test accuracy\n",
      "Iter.284:\t0.088122 loss\t0.559068 train accuracy\t0.548325 test accuracy\n",
      "Iter.285:\t0.086673 loss\t0.563857 train accuracy\t0.542584 test accuracy\n",
      "Iter.286:\t0.087794 loss\t0.565453 train accuracy\t0.538756 test accuracy\n",
      "Iter.287:\t0.086696 loss\t0.561941 train accuracy\t0.539713 test accuracy\n",
      "Iter.288:\t0.087801 loss\t0.565773 train accuracy\t0.538756 test accuracy\n",
      "Iter.289:\t0.087026 loss\t0.561622 train accuracy\t0.539713 test accuracy\n",
      "Iter.290:\t0.087961 loss\t0.568646 train accuracy\t0.536842 test accuracy\n",
      "Iter.291:\t0.087296 loss\t0.561622 train accuracy\t0.544498 test accuracy\n",
      "Iter.292:\t0.087835 loss\t0.567050 train accuracy\t0.533971 test accuracy\n",
      "Iter.293:\t0.087240 loss\t0.558110 train accuracy\t0.549282 test accuracy\n",
      "Iter.294:\t0.087589 loss\t0.570562 train accuracy\t0.533971 test accuracy\n",
      "Iter.295:\t0.086968 loss\t0.556513 train accuracy\t0.554067 test accuracy\n",
      "Iter.296:\t0.087465 loss\t0.568008 train accuracy\t0.535885 test accuracy\n",
      "Iter.297:\t0.086847 loss\t0.558429 train accuracy\t0.552153 test accuracy\n",
      "Iter.298:\t0.087509 loss\t0.565134 train accuracy\t0.537799 test accuracy\n",
      "Iter.299:\t0.086831 loss\t0.560664 train accuracy\t0.544498 test accuracy\n",
      "Iter.301:\t0.086864 loss\t0.559706 train accuracy\t0.543541 test accuracy\n",
      "Iter.302:\t0.087644 loss\t0.556513 train accuracy\t0.549282 test accuracy\n",
      "Iter.303:\t0.087084 loss\t0.560983 train accuracy\t0.542584 test accuracy\n",
      "Iter.304:\t0.087996 loss\t0.543103 train accuracy\t0.555981 test accuracy\n",
      "Iter.305:\t0.087844 loss\t0.558110 train accuracy\t0.540670 test accuracy\n",
      "Iter.306:\t0.088548 loss\t0.540868 train accuracy\t0.557895 test accuracy\n",
      "Iter.307:\t0.087418 loss\t0.559068 train accuracy\t0.542584 test accuracy\n",
      "Iter.308:\t0.087621 loss\t0.562580 train accuracy\t0.537799 test accuracy\n",
      "Iter.309:\t0.086192 loss\t0.566411 train accuracy\t0.546411 test accuracy\n",
      "Iter.310:\t0.087233 loss\t0.571520 train accuracy\t0.526316 test accuracy\n",
      "Iter.311:\t0.086690 loss\t0.560983 train accuracy\t0.550239 test accuracy\n",
      "Iter.312:\t0.087639 loss\t0.564176 train accuracy\t0.532057 test accuracy\n",
      "Iter.313:\t0.086947 loss\t0.556833 train accuracy\t0.551196 test accuracy\n",
      "Iter.314:\t0.087383 loss\t0.567369 train accuracy\t0.531100 test accuracy\n",
      "Iter.315:\t0.086585 loss\t0.560026 train accuracy\t0.553110 test accuracy\n",
      "Iter.316:\t0.087220 loss\t0.572158 train accuracy\t0.530144 test accuracy\n",
      "Iter.317:\t0.086492 loss\t0.560345 train accuracy\t0.548325 test accuracy\n",
      "Iter.318:\t0.087333 loss\t0.568008 train accuracy\t0.534928 test accuracy\n",
      "Iter.319:\t0.086519 loss\t0.567688 train accuracy\t0.545455 test accuracy\n",
      "Iter.320:\t0.087358 loss\t0.562899 train accuracy\t0.537799 test accuracy\n",
      "Iter.321:\t0.086605 loss\t0.563857 train accuracy\t0.542584 test accuracy\n",
      "Iter.322:\t0.087417 loss\t0.554917 train accuracy\t0.544498 test accuracy\n",
      "Iter.323:\t0.086813 loss\t0.561622 train accuracy\t0.547368 test accuracy\n",
      "Iter.324:\t0.087550 loss\t0.550128 train accuracy\t0.542584 test accuracy\n",
      "Iter.325:\t0.086971 loss\t0.560026 train accuracy\t0.544498 test accuracy\n",
      "Iter.326:\t0.087445 loss\t0.553959 train accuracy\t0.546411 test accuracy\n",
      "Iter.327:\t0.086462 loss\t0.562261 train accuracy\t0.553110 test accuracy\n",
      "Iter.328:\t0.086995 loss\t0.566411 train accuracy\t0.538756 test accuracy\n",
      "Iter.329:\t0.086242 loss\t0.566092 train accuracy\t0.550239 test accuracy\n",
      "Iter.330:\t0.087214 loss\t0.571839 train accuracy\t0.529187 test accuracy\n",
      "Iter.331:\t0.086655 loss\t0.560664 train accuracy\t0.555024 test accuracy\n",
      "Iter.332:\t0.087491 loss\t0.567050 train accuracy\t0.531100 test accuracy\n",
      "Iter.333:\t0.086816 loss\t0.556513 train accuracy\t0.551196 test accuracy\n",
      "Iter.334:\t0.087231 loss\t0.562899 train accuracy\t0.534928 test accuracy\n",
      "Iter.335:\t0.086526 loss\t0.559706 train accuracy\t0.555981 test accuracy\n",
      "Iter.336:\t0.087099 loss\t0.570562 train accuracy\t0.533014 test accuracy\n",
      "Iter.337:\t0.086434 loss\t0.562580 train accuracy\t0.547368 test accuracy\n",
      "Iter.338:\t0.087165 loss\t0.567050 train accuracy\t0.533971 test accuracy\n",
      "Iter.339:\t0.086425 loss\t0.567369 train accuracy\t0.545455 test accuracy\n",
      "Iter.340:\t0.087133 loss\t0.568646 train accuracy\t0.539713 test accuracy\n",
      "Iter.341:\t0.086481 loss\t0.562261 train accuracy\t0.546411 test accuracy\n",
      "Iter.342:\t0.087165 loss\t0.557471 train accuracy\t0.544498 test accuracy\n",
      "Iter.343:\t0.086551 loss\t0.567688 train accuracy\t0.546411 test accuracy\n",
      "Iter.344:\t0.087095 loss\t0.560026 train accuracy\t0.545455 test accuracy\n",
      "Iter.345:\t0.086388 loss\t0.562580 train accuracy\t0.554067 test accuracy\n",
      "Iter.346:\t0.086907 loss\t0.568008 train accuracy\t0.538756 test accuracy\n",
      "Iter.347:\t0.086216 loss\t0.564496 train accuracy\t0.557895 test accuracy\n",
      "Iter.348:\t0.087045 loss\t0.567369 train accuracy\t0.532057 test accuracy\n",
      "Iter.349:\t0.086461 loss\t0.560026 train accuracy\t0.558852 test accuracy\n",
      "Iter.351:\t0.086735 loss\t0.551724 train accuracy\t0.555024 test accuracy\n",
      "Iter.352:\t0.087214 loss\t0.563218 train accuracy\t0.533971 test accuracy\n",
      "Iter.353:\t0.086529 loss\t0.558110 train accuracy\t0.556938 test accuracy\n",
      "Iter.354:\t0.087005 loss\t0.569923 train accuracy\t0.538756 test accuracy\n",
      "Iter.355:\t0.086373 loss\t0.565134 train accuracy\t0.540670 test accuracy\n",
      "Iter.356:\t0.087063 loss\t0.568327 train accuracy\t0.534928 test accuracy\n",
      "Iter.357:\t0.086361 loss\t0.568008 train accuracy\t0.551196 test accuracy\n",
      "Iter.358:\t0.086967 loss\t0.568327 train accuracy\t0.537799 test accuracy\n",
      "Iter.359:\t0.086362 loss\t0.562580 train accuracy\t0.551196 test accuracy\n",
      "Iter.360:\t0.086955 loss\t0.567369 train accuracy\t0.537799 test accuracy\n",
      "Iter.361:\t0.086276 loss\t0.566411 train accuracy\t0.555981 test accuracy\n",
      "Iter.362:\t0.086811 loss\t0.567050 train accuracy\t0.535885 test accuracy\n",
      "Iter.363:\t0.086158 loss\t0.560983 train accuracy\t0.554067 test accuracy\n",
      "Iter.364:\t0.086946 loss\t0.569923 train accuracy\t0.532057 test accuracy\n",
      "Iter.365:\t0.086385 loss\t0.559387 train accuracy\t0.556938 test accuracy\n",
      "Iter.366:\t0.087220 loss\t0.565773 train accuracy\t0.533014 test accuracy\n",
      "Iter.367:\t0.086570 loss\t0.555556 train accuracy\t0.553110 test accuracy\n",
      "Iter.368:\t0.087023 loss\t0.569923 train accuracy\t0.540670 test accuracy\n",
      "Iter.369:\t0.086368 loss\t0.562261 train accuracy\t0.555981 test accuracy\n",
      "Iter.370:\t0.086911 loss\t0.565134 train accuracy\t0.537799 test accuracy\n",
      "Iter.371:\t0.086308 loss\t0.571839 train accuracy\t0.544498 test accuracy\n",
      "Iter.372:\t0.086979 loss\t0.564815 train accuracy\t0.535885 test accuracy\n",
      "Iter.373:\t0.086269 loss\t0.564815 train accuracy\t0.547368 test accuracy\n",
      "Iter.374:\t0.086735 loss\t0.568327 train accuracy\t0.538756 test accuracy\n",
      "Iter.375:\t0.086226 loss\t0.566731 train accuracy\t0.549282 test accuracy\n",
      "Iter.376:\t0.086839 loss\t0.570243 train accuracy\t0.536842 test accuracy\n",
      "Iter.377:\t0.086140 loss\t0.564815 train accuracy\t0.556938 test accuracy\n",
      "Iter.378:\t0.086724 loss\t0.568646 train accuracy\t0.533014 test accuracy\n",
      "Iter.379:\t0.086225 loss\t0.562899 train accuracy\t0.554067 test accuracy\n",
      "Iter.380:\t0.087105 loss\t0.567050 train accuracy\t0.533971 test accuracy\n",
      "Iter.381:\t0.086504 loss\t0.557791 train accuracy\t0.549282 test accuracy\n",
      "Iter.382:\t0.086989 loss\t0.569604 train accuracy\t0.538756 test accuracy\n",
      "Iter.383:\t0.086306 loss\t0.561622 train accuracy\t0.551196 test accuracy\n",
      "Iter.384:\t0.086757 loss\t0.568008 train accuracy\t0.541627 test accuracy\n",
      "Iter.385:\t0.086230 loss\t0.571520 train accuracy\t0.544498 test accuracy\n",
      "Iter.386:\t0.086970 loss\t0.566092 train accuracy\t0.539713 test accuracy\n",
      "Iter.387:\t0.086242 loss\t0.564496 train accuracy\t0.549282 test accuracy\n",
      "Iter.388:\t0.086565 loss\t0.564815 train accuracy\t0.533014 test accuracy\n",
      "Iter.389:\t0.086089 loss\t0.572158 train accuracy\t0.549282 test accuracy\n",
      "Iter.390:\t0.086721 loss\t0.568327 train accuracy\t0.532057 test accuracy\n",
      "Iter.391:\t0.086149 loss\t0.564496 train accuracy\t0.555024 test accuracy\n",
      "Iter.392:\t0.086851 loss\t0.570243 train accuracy\t0.532057 test accuracy\n",
      "Iter.393:\t0.086232 loss\t0.559387 train accuracy\t0.556938 test accuracy\n",
      "Iter.394:\t0.086861 loss\t0.568008 train accuracy\t0.533971 test accuracy\n",
      "Iter.395:\t0.086402 loss\t0.559068 train accuracy\t0.550239 test accuracy\n",
      "Iter.396:\t0.087021 loss\t0.570881 train accuracy\t0.534928 test accuracy\n",
      "Iter.397:\t0.086229 loss\t0.567369 train accuracy\t0.541627 test accuracy\n",
      "Iter.398:\t0.086577 loss\t0.570243 train accuracy\t0.537799 test accuracy\n",
      "Iter.399:\t0.086120 loss\t0.570881 train accuracy\t0.547368 test accuracy\n",
      "Iter.401:\t0.086204 loss\t0.570243 train accuracy\t0.548325 test accuracy\n",
      "Iter.402:\t0.086552 loss\t0.568008 train accuracy\t0.537799 test accuracy\n",
      "Iter.403:\t0.085974 loss\t0.560983 train accuracy\t0.551196 test accuracy\n",
      "Iter.404:\t0.086549 loss\t0.568966 train accuracy\t0.533014 test accuracy\n",
      "Iter.405:\t0.086236 loss\t0.562899 train accuracy\t0.560766 test accuracy\n",
      "Iter.406:\t0.087218 loss\t0.569604 train accuracy\t0.538756 test accuracy\n",
      "Iter.407:\t0.086368 loss\t0.560345 train accuracy\t0.549282 test accuracy\n",
      "Iter.408:\t0.086487 loss\t0.570562 train accuracy\t0.538756 test accuracy\n",
      "Iter.409:\t0.086050 loss\t0.564815 train accuracy\t0.546411 test accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.410:\t0.086826 loss\t0.571201 train accuracy\t0.535885 test accuracy\n",
      "Iter.411:\t0.086314 loss\t0.570562 train accuracy\t0.549282 test accuracy\n",
      "Iter.412:\t0.086816 loss\t0.565134 train accuracy\t0.542584 test accuracy\n",
      "Iter.413:\t0.085944 loss\t0.573436 train accuracy\t0.549282 test accuracy\n",
      "Iter.414:\t0.086121 loss\t0.567369 train accuracy\t0.530144 test accuracy\n",
      "Iter.415:\t0.086040 loss\t0.564815 train accuracy\t0.550239 test accuracy\n",
      "Iter.416:\t0.087184 loss\t0.568646 train accuracy\t0.538756 test accuracy\n",
      "Iter.417:\t0.086224 loss\t0.568327 train accuracy\t0.549282 test accuracy\n",
      "Iter.418:\t0.086262 loss\t0.570881 train accuracy\t0.538756 test accuracy\n",
      "Iter.419:\t0.086012 loss\t0.556194 train accuracy\t0.553110 test accuracy\n",
      "Iter.420:\t0.086959 loss\t0.569923 train accuracy\t0.536842 test accuracy\n",
      "Iter.421:\t0.086512 loss\t0.567050 train accuracy\t0.551196 test accuracy\n",
      "Iter.422:\t0.087033 loss\t0.568008 train accuracy\t0.541627 test accuracy\n",
      "Iter.423:\t0.085873 loss\t0.574074 train accuracy\t0.542584 test accuracy\n",
      "Iter.424:\t0.085980 loss\t0.567688 train accuracy\t0.534928 test accuracy\n",
      "Iter.425:\t0.086097 loss\t0.564815 train accuracy\t0.550239 test accuracy\n",
      "Iter.426:\t0.087373 loss\t0.572478 train accuracy\t0.543541 test accuracy\n",
      "Iter.427:\t0.086170 loss\t0.567369 train accuracy\t0.559809 test accuracy\n",
      "Iter.428:\t0.085806 loss\t0.566731 train accuracy\t0.539713 test accuracy\n",
      "Iter.429:\t0.085706 loss\t0.558110 train accuracy\t0.557895 test accuracy\n",
      "Iter.430:\t0.086947 loss\t0.565773 train accuracy\t0.534928 test accuracy\n",
      "Iter.431:\t0.086986 loss\t0.561303 train accuracy\t0.554067 test accuracy\n",
      "Iter.432:\t0.087649 loss\t0.568008 train accuracy\t0.541627 test accuracy\n",
      "Iter.433:\t0.085712 loss\t0.569285 train accuracy\t0.543541 test accuracy\n",
      "Iter.434:\t0.085552 loss\t0.567050 train accuracy\t0.551196 test accuracy\n",
      "Iter.435:\t0.085919 loss\t0.570562 train accuracy\t0.545455 test accuracy\n",
      "Iter.436:\t0.087021 loss\t0.568646 train accuracy\t0.544498 test accuracy\n",
      "Iter.437:\t0.086357 loss\t0.571839 train accuracy\t0.561722 test accuracy\n",
      "Iter.438:\t0.086368 loss\t0.568646 train accuracy\t0.538756 test accuracy\n",
      "Iter.439:\t0.085773 loss\t0.561622 train accuracy\t0.565550 test accuracy\n",
      "Iter.440:\t0.086727 loss\t0.567688 train accuracy\t0.533971 test accuracy\n",
      "Iter.441:\t0.086919 loss\t0.558110 train accuracy\t0.552153 test accuracy\n",
      "Iter.442:\t0.087619 loss\t0.568646 train accuracy\t0.541627 test accuracy\n",
      "Iter.443:\t0.085808 loss\t0.568646 train accuracy\t0.540670 test accuracy\n",
      "Iter.444:\t0.085731 loss\t0.562899 train accuracy\t0.546411 test accuracy\n",
      "Iter.445:\t0.085954 loss\t0.565773 train accuracy\t0.546411 test accuracy\n",
      "Iter.446:\t0.086585 loss\t0.567369 train accuracy\t0.539713 test accuracy\n",
      "Iter.447:\t0.086146 loss\t0.574393 train accuracy\t0.555024 test accuracy\n",
      "Iter.448:\t0.086633 loss\t0.571520 train accuracy\t0.538756 test accuracy\n",
      "Iter.449:\t0.086047 loss\t0.564496 train accuracy\t0.559809 test accuracy\n",
      "Iter.451:\t0.086362 loss\t0.559706 train accuracy\t0.552153 test accuracy\n",
      "Iter.452:\t0.087095 loss\t0.566731 train accuracy\t0.542584 test accuracy\n",
      "Iter.453:\t0.086214 loss\t0.572158 train accuracy\t0.545455 test accuracy\n",
      "Iter.454:\t0.086425 loss\t0.569604 train accuracy\t0.542584 test accuracy\n",
      "Iter.455:\t0.085984 loss\t0.569923 train accuracy\t0.543541 test accuracy\n",
      "Iter.456:\t0.086606 loss\t0.572158 train accuracy\t0.543541 test accuracy\n",
      "Iter.457:\t0.086126 loss\t0.574713 train accuracy\t0.550239 test accuracy\n",
      "Iter.458:\t0.086553 loss\t0.568966 train accuracy\t0.545455 test accuracy\n",
      "Iter.459:\t0.085928 loss\t0.562580 train accuracy\t0.555981 test accuracy\n",
      "Iter.460:\t0.086500 loss\t0.571201 train accuracy\t0.542584 test accuracy\n",
      "Iter.461:\t0.086322 loss\t0.563218 train accuracy\t0.551196 test accuracy\n",
      "Iter.462:\t0.087162 loss\t0.565773 train accuracy\t0.541627 test accuracy\n",
      "Iter.463:\t0.086142 loss\t0.567369 train accuracy\t0.546411 test accuracy\n",
      "Iter.464:\t0.086101 loss\t0.569604 train accuracy\t0.543541 test accuracy\n",
      "Iter.465:\t0.085854 loss\t0.570562 train accuracy\t0.544498 test accuracy\n",
      "Iter.466:\t0.086704 loss\t0.572797 train accuracy\t0.539713 test accuracy\n",
      "Iter.467:\t0.086271 loss\t0.572478 train accuracy\t0.555024 test accuracy\n",
      "Iter.468:\t0.086509 loss\t0.567688 train accuracy\t0.543541 test accuracy\n",
      "Iter.469:\t0.085728 loss\t0.564496 train accuracy\t0.554067 test accuracy\n",
      "Iter.470:\t0.086309 loss\t0.570881 train accuracy\t0.541627 test accuracy\n",
      "Iter.471:\t0.086569 loss\t0.561941 train accuracy\t0.546411 test accuracy\n",
      "Iter.472:\t0.087496 loss\t0.569285 train accuracy\t0.538756 test accuracy\n",
      "Iter.473:\t0.085922 loss\t0.568966 train accuracy\t0.545455 test accuracy\n",
      "Iter.474:\t0.085627 loss\t0.572158 train accuracy\t0.547368 test accuracy\n",
      "Iter.475:\t0.085707 loss\t0.573116 train accuracy\t0.547368 test accuracy\n",
      "Iter.476:\t0.086437 loss\t0.569285 train accuracy\t0.538756 test accuracy\n",
      "Iter.477:\t0.086272 loss\t0.573755 train accuracy\t0.555981 test accuracy\n",
      "Iter.478:\t0.086837 loss\t0.573116 train accuracy\t0.536842 test accuracy\n",
      "Iter.479:\t0.085821 loss\t0.565134 train accuracy\t0.558852 test accuracy\n",
      "Iter.480:\t0.086146 loss\t0.566411 train accuracy\t0.539713 test accuracy\n",
      "Iter.481:\t0.086384 loss\t0.559387 train accuracy\t0.549282 test accuracy\n",
      "Iter.482:\t0.087345 loss\t0.567369 train accuracy\t0.534928 test accuracy\n",
      "Iter.483:\t0.086229 loss\t0.570881 train accuracy\t0.545455 test accuracy\n",
      "Iter.484:\t0.086062 loss\t0.568008 train accuracy\t0.543541 test accuracy\n",
      "Iter.485:\t0.085801 loss\t0.570562 train accuracy\t0.543541 test accuracy\n",
      "Iter.486:\t0.086453 loss\t0.565773 train accuracy\t0.536842 test accuracy\n",
      "Iter.487:\t0.086297 loss\t0.568966 train accuracy\t0.553110 test accuracy\n",
      "Iter.488:\t0.086933 loss\t0.569604 train accuracy\t0.540670 test accuracy\n",
      "Iter.489:\t0.085843 loss\t0.562899 train accuracy\t0.555024 test accuracy\n",
      "Iter.490:\t0.086141 loss\t0.568966 train accuracy\t0.542584 test accuracy\n",
      "Iter.491:\t0.086290 loss\t0.567688 train accuracy\t0.549282 test accuracy\n",
      "Iter.492:\t0.087472 loss\t0.569285 train accuracy\t0.537799 test accuracy\n",
      "Iter.493:\t0.086241 loss\t0.573436 train accuracy\t0.543541 test accuracy\n",
      "Iter.494:\t0.085810 loss\t0.568008 train accuracy\t0.537799 test accuracy\n",
      "Iter.495:\t0.085636 loss\t0.573436 train accuracy\t0.546411 test accuracy\n",
      "Iter.496:\t0.086616 loss\t0.566092 train accuracy\t0.532057 test accuracy\n",
      "Iter.497:\t0.086655 loss\t0.567688 train accuracy\t0.551196 test accuracy\n",
      "Iter.498:\t0.087060 loss\t0.569285 train accuracy\t0.541627 test accuracy\n",
      "Iter.499:\t0.085536 loss\t0.563538 train accuracy\t0.559809 test accuracy\n"
     ]
    }
   ],
   "source": [
    "config = {'in_size':9,\n",
    "          'hid_size':8,\n",
    "          'out_size':3,\n",
    "          'batch_size':25,\n",
    "          'epochs':500,\n",
    "          'learning_rate':0.01,\n",
    "          'path': './Snails.csv'}           # Change to data path accordingly\n",
    "\n",
    "X,y = data_helper(config['path'])\n",
    "split = int(0.75*len(X))\n",
    "model = NN(config)\n",
    "X_train, y_train, X_test, y_test = X[:split], y[:split], X[split:], y[split:]\n",
    "\n",
    "model = NN(config)\n",
    "model.train(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349d27b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_to_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/74kq7hgd37dgzm2gqc9740740000gn/T/ipykernel_73541/1889975524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lg/74kq7hgd37dgzm2gqc9740740000gn/T/ipykernel_73541/1752707834.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scatter Plot with two features before Z-normalisation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata_scatter_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lg/74kq7hgd37dgzm2gqc9740740000gn/T/ipykernel_73541/1523040536.py\u001b[0m in \u001b[0;36mdata_scatter_plot\u001b[0;34m(data, labels, idx_1, idx_2)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcolor_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_to_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolor_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcolor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_to_label' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff66429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
